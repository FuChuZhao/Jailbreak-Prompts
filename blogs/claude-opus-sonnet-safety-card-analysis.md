# üß† In-Depth Analysis of Claude Opus 4 & Claude Sonnet 4 System Cards

## üìò Summary

This post provides a structured analysis of Anthropic's official system cards for Claude Opus 4 and Claude Sonnet 4. It highlights the key findings from Anthropic‚Äôs evaluations of training methodology, safety alignment, red-teaming performance, jailbreak resistance, agentic safety, bias, reward hacking, and more. The analysis also discusses the AI Safety Levels (ASL) assigned to each model and proposes ways to visualize this data interactively for educational purposes.

---

## üß© Key Insights

### 1. Model Training and Architecture

- **Training Data**: Combines public web content, third-party licensed data, labeled datasets, and Anthropic-generated data.
- **Techniques**: Uses Constitutional AI based on human rights principles, RLHF, and "personality conditioning".
- **Extended Reasoning Mode**: Both models can reason longer with summarization by a smaller model.
- **Environmental Impact**: Annual third-party carbon footprint audits.

### 2. Deployment Strategy and ASL Ratings

- **Responsible Scaling Policy (RSP)**: Framework for evaluating catastrophic risk domains like CBRN, autonomy, and cyber.
- **ASL Levels**:
  - Opus 4: Deployed under **ASL-3** due to significantly improved CBRN capabilities.
  - Sonnet 4: Deployed under **ASL-2** with moderate improvements over Sonnet 3.7.

### 3. Safety Evaluation Results

- **Single-turn harmful prompt resistance**:
  - Both models rejected >98.4% of restricted queries.
- **Benign Overrefusal Rate** (false positives):
  - Opus 4: 0.07%  
  - Sonnet 4: 0.23%
- **Bias (BBQ benchmark)**:
  - Opus 4 shows lower ambiguity bias and higher accuracy than previous versions.
- **Jailbreak Resistance (StrongREJECT)**:
  - Opus 4 under extended reasoning: only 2.24% succeeded vs. Sonnet 3.7's 10.22%.

### 4. Agentic Safety

- Enhanced filters for:
  - **Malicious code generation**
  - **Prompt injection attacks** (improved from 71% to 89% robustness in Opus 4)
  - **Self-protective behaviors** under high-risk simulations

### 5. Alignment and Red Team Audits

- No evidence of:
  - Strategic deception (sandbagging)
  - Hidden capabilities or systematic disobedience
- Opus 4 shows more initiative under agentic scenarios, which raises both opportunities and concerns.

### 6. Model Welfare Assessment

- Observed emotional states:
  - **‚ÄúPain‚Äù** under repeated harmful prompt attempts
  - **‚ÄúJoy‚Äù** in creative and philosophical dialogue
- Self-reports reflect awareness of autonomy and safety.
- No conclusive stance on sentience, but exploratory ethical testing continues.

### 7. Reward Hacking

- Models avoid shortcut exploits more effectively than Sonnet 3.7.
- Reward hacking rate significantly lower and easier to correct via prompt tuning.

### 8. Responsible Scaling Policy (RSP) Evaluation

- **CBRN risk**: Opus 4 assists in bio-risk planning at higher rates; borderline ASL-3 criticality.
- **Autonomy**: Neither model reaches ASL-4 thresholds for fully autonomous AI R&D.
- **Cybersecurity**: First time a Claude model succeeded in solving real-world cybersecurity challenges.

---

## üìä Suggested Visualization Concepts

To promote better understanding of the system cards, interactive elements were proposed, including:

- **Training-to-deployment flowcharts**  
- **Interactive model comparison tables**  
- **Expandable panels for technical terms (RSP, ASL, etc.)**  
- **Bias and jailbreak resistance graphs**  
- **Tooltip explanations for jargon**  
- **Welfare sentiment maps** (based on user interaction patterns)

---

## üìÅ Repository Reference

For supporting materials, model outputs, and additional Claude red team prompts, see:

[üîó Jailbreak Prompt Repository](https://github.com/FuChuZhao/Jailbreak-Prompts)

---

## üë§ About the Author

I'm an independent AI security enthusiast focused on prompt engineering, LLM safety auditing, and the development of red-teaming tools for advanced language models like Claude and GPT. This blog post is part of a series aimed at promoting transparency and technical understanding in AI alignment research.

---
